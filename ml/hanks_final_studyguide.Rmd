---
title: "hanks_final_prep"
output: pdf_document
date: "2023-04-13"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
options(scipen = 999)
```

# FINAL EXAM PREPARATION 

The purpose of this document is to consolidate all pertinent theory, code, and examples in one place to have on hand for final exam. The final exam is on Tuesday, April 18. 

wine_words function for text mining description: 
```{r}
wine_words <- function(df, j = 1000, stem=F){ 
  library(tidytext)
  library(SnowballC)
  data(stop_words)

  words <- df %>%
    unnest_tokens(word, description) %>%
    anti_join(stop_words) %>% # get rid of stop words
    filter(!(word %in% c("wine","pinot","vineyard")))
  
  if(stem){
    words <- words %>% 
      mutate(word = wordStem(word))
  }
  
  words <- words %>% 
    count(id, word) %>% 
    group_by(id) %>% 
    mutate(exists = (n>0)) %>% 
    ungroup %>% 
    group_by(word) %>% 
    mutate(total = sum(n)) %>% 
    filter(total > j) %>% 
    pivot_wider(id_cols = id, names_from = word, values_from = exists, values_fill = list(exists=0)) %>% 
    right_join(select(df,id,province)) %>% 
    #drop_na() %>% 
    select(-id)
  
  
  words <- replace(words, is.na(words), FALSE)
}
```

## 1. Feature Engineering
### Encoding categorical variables
fastDummies package

example: 
```{r}
wine %>%
select(variety) %>%
mutate(variety=fct_lump(variety,4)) %>%
dummy_cols() %>%
head()
```



### Interactions:

" two or more predictors are said to interact if their combined effect is different (less or greater) than what we would expect if we were to add the impact of each of their effects when considered alone."

<http://www.feat.engineering/detecting-interaction-effects.html>

• Transformations

### BoxCox 
Box-Cox transformations use MLE to estimate λ
• when λ = 1, there is no transformation (shift by 1)
• when λ = 0, it is log transformed (special case)
• when λ = 0.5, it is square root
• when λ = −1, it is an inverse

```{r}
wine %>%
preProcess(method = c("BoxCox","center","scale")) %>%
predict(wine) %>%
select(-description) %>%
head()
```



### Interpreting coefficients given log transformations

**When only the dependent/response variable is log-transformed:**
Exponentiate the coefficient, subtract one from this number, and multiply by 100. This gives the percent increase (or decrease) in the response for every one-unit increase in the independent variable. Example: the coefficient is 0.198. (exp(0.198) – 1)* 100 = 21.9. For every one-unit increase in the independent variable, our dependent variable increases
by about 22%

Example: 
```{r}
model <- lm(lprice~points, filter(wine,province=="Oregon"))
print(model)

pct = (exp(coef(model)["points"]) - 1) * 100
```

We have got to exponentiate the coefficient of points: 
```{r}
exp(.094)
```

a 1 point rating increase equals a 9.85% increase in price on average:

Formula: 
 (e^x - 1)*100
 
```{r}
(exp(.094) -1 )*100
```

as code: 
```{r}

for(v in c("Chardonnay", "Pinot Gris","Pinot Noir")){
m <- lm(lprice~points, filter(wine,province=="Oregon", variety==v))
pct <- round((exp(coef(m)["points"]) - 1) * 100,2)
print(str_c("For ",v,", a 1 point ratings increase leads to a ",pct,"% increase in price."))
}

```


**Only independent/predictor variable(s) is log-transformed** Divide the coefficient by 100. This tells
us that a 1% increase in the independent variable increases (or decreases) the dependent variable by
(coefficient/100) units. Example: the coefficient is 0.198. 0.198/100 = 0.00198. For every 1% increase
in the independent variable, our dependent variable increases by about 0.002


When the feature (IV) is logged: 

```{r}
model <- lm(price~lpoints, filter(wine,province=="Oregon") %>% mutate(lpoints=log(points)))

model
```
```{r}
coef(model)["lpoints"]/100
```

Now we look at an increase in the feature.
A 1% ratings increase means the dependent variable increases by coefficient/100. 



# Standardizing

*Centering* is basically a technique where mean of independent variables is subtracted from all the values. It means all independent variables have zero mean. 

*Scaling* is similar to centering. Predictor variables are divided by their standard deviation. This way data will have a standard deviation of one.

Standardizing 
- mean-centering: x - x_bar (taking each value and subtracting the average from it) 
- scaling x / sd(x) (dividing each value by the standard deviation)

Demonstrating what scale() does manually 
```{r}
#creating a data frame of random numbers 
df = data.frame(x = sample(1:1000,100), y = sample(1:1000,100))

#saving the average of x to a variable m:
m = mean(df$x)

#creating a column of centered x, where we subtract each value by the average of the col 
df = df %>% mutate(x.cent = x-m)

#dividing that column by the standard deviation 
df = df %>% mutate(x.cent.scaled = x.cent/sd(x.cent))

#comparing our manual scale and center x with the function scale() 
df= df %>% mutate(xsc = scale(x,center = TRUE, scale = TRUE), 
                  ysc = scale(y, center = TRUE, scale = TRUE)) 

#they are the same. 

#let's visualize this 
ggplot(df, aes(x = x, y = y)) + geom_point(color = "red") #very random!
ggplot(df, aes(x = xsc, y = ysc)) + geom_point(color = "blue")

```



## 2. KNN
• How does it work?

“The k-nearest neighbors (KNN) algorithm is a data classification method for estimating the likelihood that a data point will become a member of one group or another based on what group the data points nearest to it belong to”

• In *k-NN classification*, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its *k* nearest neighbors (*k* is a positive [integer](https://en.wikipedia.org/wiki/Integer), typically small). If *k* = 1, then the object is simply assigned to the class of that single nearest neighbor.

• In *k-NN regression*, the output is the property value for the object. This value is the average of the values of *k* nearest neighbors. If *k* = 1, then the output is simply assigned to the value of that single nearest neighbor.

It's important that all variables are scaled and centered because Knn is based on geometric distances between data points in n-dimensional space. 

Algorithm
1. Load the data
2. Initialize K to your chosen number of neighbors
3. For each example in the data
4. Calculate the distance between the query example and the current example from the data.
5. Add the distance and the index of the example to an ordered collection
6. Sort the ordered collection of distances and indices from smallest to largest (in ascending order) by the
distances
7. Pick the first K entries from the sorted collection
8. Get the labels of the selected K entries
9. If regression, return the mean of the K labels
10. If classification, return the mode of the K labels

• Accuracy vs. Kappa

Kappa accuracy statistic
• We saw that accuracy isn’t always good for evaluating classifiers.
• κ gives us another measure by comparing our model to “expected accuracy” = how well a classifier
would do by chance.
• κ divides the difference by the difference between a perfect model and expected accuracy.


• Leakage: when a variable somehow indicates the response variable. 

• Tuning

making a tuning plot: 
```{r}
ggplot(fit, metric="Kappa")
```


• Classification vs. Regression
• Confusion Matrix
```{r}
confusionMatrix(predict(fit, test),factor(test$province))
```



## 3. Naive Bayes
• How does it work?

$$ P(L~|~{\rm features}) = \frac{P({\rm features}~|~L)P(L)}{P({\rm features})} $$



$$ P({\rm Thing 1}~|~{\rm Thing 2}) = \frac{P({\rm Thing 2}~|~ \rm{Thing 1})P(\rm{Thing 1})}{P({\rm Thing 2})} $$

• What is a conditional probability?


## 4. Tidytext

• Document-term matrix

```{r}
library(tidytext)
data("stop_words")

```

Creating a document term matrix: 

```{r}
wine$ID = wine$id
wine = wine %>% select(-id)


df = wine %>% 
  unnest_tokens(word, description) %>% # output is word column, input is description column
  anti_join(stop_words) %>% #removing stopwords from word col 
  filter(word !="wine") %>% filter(word != "pinot") %>% 
  count(ID, word) %>% 
  group_by(ID) %>% 
  mutate(freq = n/sum(n)) %>% # frequency of that word
  mutate(exists = (n>0)) %>% ungroup %>% 
  group_by(word) %>% 
  mutate(total = sum(n)) # how many times that word is used in entire ds

```
• tokens, stems


## 5. Logistic Regression
• How does it work?
• Why is it called regression and not classification?

Because the output of a logistic regression model is a probability. 

• ROC curves
Measure of different specificity / sensitivities across a spectrum of cut off points for making prediction of class. 

• Regularization
Prevents overfitting

## 6. Decision Trees
• How do they work?
• What is information gain?
• What is the complexity parameter?
• What does weighting the classes accomplish?
• How do random forests work?
• What are the strengths / weaknesses?

Random forests models are solid for both regression and classification problems. They handle outliers well (as long as the training process includes cross validation). They are good for gather information on feature importance. There are multiple hyperparameters to tune get great performance. 

They can be computationally expensive, especially with a high ntree. 

• Random forest vs. bagged trees


## 7. Dimensionality Reduction
• What is curse of dimensionality?

Distances between observations tend to shrink as dimensionality tends to infinity. 

cgpt: 
"As the number of features or dimensions of the data increases, the amount of data required to accurately represent the distribution of the data increases exponentially. This leads to several problems, such as increased computational complexity, overfitting, and decreased predictive performance.

In high-dimensional spaces, the distance between any two points tends to become almost the same, which can make it difficult to identify patterns or make accurate predictions. As a result, it becomes more difficult to find meaningful relationships among the data points, and the accuracy of machine learning models tends to decrease as the number of dimensions increases.

To overcome the curse of dimensionality, various techniques are used, such as feature selection, feature extraction, and dimensionality reduction. These techniques help to reduce the dimensionality of the data while preserving the most important information, thereby improving the performance of machine learning models."

• K-means
• Analysis of cluster centers, density plots
• Hierarchical
• Principle Component Analysis
• Factor loadings, interpretation, density plots
• Choosing number of clusters, choosing number of PCs to use


## 8. Clustering
• Cluster centers, interpretation
• Algorithm
• LDA: when and why?

Latent Dirichlet Allocation: a mathematical model for estimating topics - finding a mixture of words that is associated with each topic, while also determining the mixture of topics that describes each document. 
- a method used in text mining? 
- we can get a per-topic-per-word probabilities 
<https://www.tidytextmining.com/topicmodeling.html#latent-dirichlet-allocation>


## 9. Deep Learning
• Nodes
• Weights
• Activation Function

## 10. Meta-techniques
• Bagging
Bagging = "boostrap aggregating" 

Bagging combines and averages multiple models. Averaging across multiple trees reduces the variability of any one tree and reduces overfitting, which improves predictive performance. Bagging follows three simple steps:

Create m bootstrap samples from the training data. Bootstrapped samples allow us to create many slightly different data sets but with the same distribution as the overall training set.
For each bootstrap sample train a single, unpruned regression tree.
Average individual predictions from each tree to create an overall average predicted value.

Bagging provides the greatest improvement for models that have high variance. 

how to bag in caret: 
```{r}
ctrl <- trainControl(method = "cv",  number = 10) 

# CV bagged model
bagged_cv <- train(
  Sale_Price ~ .,
  data = ames_train,
  method = "treebag", #treebag 
  trControl = ctrl,
  importance = TRUE
  )
```



• Boosting: GBM, what is it and how does it differ from random forest / treebag?

## GBM 

A boosted gradient model is a type of machine learning model that uses an ensemble of weak prediction models, such as decision trees, to create a stronger, more accurate prediction model. The term "boosted" refers to the fact that each subsequent model in the ensemble is built on the errors of the previous models. 

you can get a feature importance plot from gbm using `summary(fit$finalModel, cbars = 10, method = relative.influence, las = 2)`

*XGBoost* is an optimization of standard gradient boosting. It has fancy techniques for
• approximating optimal splits of continuous features;
• parallel processing of model training;
• handling sparse matrices and smart use of memory and cores;
• sophisticated built-in regularization.

XGBoost only works with matrices that contain all numeric variables.

Whereas random forests build an ensemble of deep independent trees, GBMs build an ensemble of shallow and weak successive trees with each tree learning and improving on the previous.

Advantages and Disadvantages of Gradient Boosting Models:  

Advantages:

Often provides predictive accuracy that cannot be beat.
Lots of flexibility - can optimize on different loss functions and provides several hyperparameter tuning options that make the function fit very flexible.
No data pre-processing required - often works great with categorical and numerical values as is.
Handles missing data - imputation not required.

Disadvantages: 

GBMs will continue improving to minimize all errors. This can overemphasize outliers and cause overfitting. Must use cross-validation to neutralize.
Computationally expensive - GBMs often require many trees (>1000) which can be time and memory exhaustive.
The high flexibility results in many parameters that interact and influence heavily the behavior of the approach (number of iterations, tree depth, regularization parameters, etc.). This requires a large grid search during tuning.
Less interpretable although this is easily addressed with various tools (variable importance, partial dependence plots, LIME, etc.).

The main idea is to add new models to the ensemble sequentially. We use 'weak learners- that is, decision trees that are shallow (few splits). The benefits are: 

Speed: Constructing weak models is computationally cheap.
Accuracy improvement: Weak models allow the algorithm to learn slowly; making minor adjustments in new areas where it does not perform well. In general, statistical approaches that learn slowly tend to perform well.
Avoids overfitting: Due to making only small incremental improvements with each model in the ensemble, this allows us to stop the learning process as soon as overfitting has been detected (typically by using cross-validation).


• Ensembles: how and why?

many models where all the outcomes are averaged (regression) or plurality voted (classification)
## 11. Tidy Models
• Basic usage

```{r}
library(tidymodels)
#procedures for splitting train/test data 
mpg_split = initial_split(mpg, prop = 3/4, strata = hwy)

mpg_training = mpg_split %>% training()
mpg_test = mpg_split %>% testing()

```

- On the split = mapping: 
Since the original data is split at random, stratification avoids placing all the expensive homes in home_sales into the test dataset, for example. In this case, your model would most likely perform poorly because it was trained on less expensive homes.

linear regression model: 

parsnip: 
1. model type (linear regression, etc. )
2. set engine type 
3. specify the mode(either regression or classifcation)

```{r}
lm_model = linear_reg() %>% set_engine('lm') %>% set_mode('regression')
```

pass lm_model to the fit() function: 
```{r}
lm_fit = lm_model %>% fit(hwy ~ cty, data = mpg_training)
```

pass into tidy() to create summary table of coefficients 
```{r}
tidy(lm_fit)
```

make predictions using model: 
```{r}
hwy_predictions = lm_fit %>% predict(new_data = mpg_test)
```

add predictions to the test data 
```{r}
mpg_test_results = mpg_test %>% select(hwy, cty) %>% bind_cols(hwy_predictions)
```

Evaluating model performance:  
ALl yardstick functions require a tibble 
like this: 
```{r}
mpg_test_results
```
rmse and R^2
```{r}
mpg_test_results %>% rmse(truth = hwy, estimate = .pred)

mpg_test_results %>% rsq(truth = hwy, estimate = .pred)
```
last_fit()
```{r}
lm_last_fit = lm_model %>% last_fit(hwy ~ cty, split = mpg_split)

lm_last_fit %>% collect_metrics()

lm_last_fit %>% collect_predictions()
```

```{r}
# Create an R squared plot of model performance
ggplot(home_test_results, aes(x = selling_price, y = .pred)) +
  geom_point(alpha = 0.5) + 
  geom_abline(color = 'blue', linetype = 2) +
  coord_obs_pred() +
  labs(x = 'Actual Home Selling Price', y = 'Predicted Selling Price')
```

```{r}
# Define a linear regression model
linear_model <- linear_reg() %>% 
  set_engine('lm') %>% 
  set_mode('regression')

# Train linear_model with last_fit()
linear_fit <- linear_model %>% 
  last_fit(selling_price ~ ., split = home_split)

# Collect predictions and view results
predictions_df <- linear_fit %>% collect_predictions()
predictions_df
                                        
# Make an R squared plot using predictions_df
ggplot(predictions_df, aes(x = selling_price, y = .pred)) + 
  geom_point(alpha = 0.5) + 
  geom_abline(color = 'blue', linetype = 2) +
  coord_obs_pred() +
  labs(x = 'Actual Home Selling Price', y = 'Predicted Selling Price')
```

## Tidymodels: classification example (datacamp)

```{r}
leads_split = initial_split(leads_df, prop = .75, strata = purchased)


#creating the model 
logistic_model = logistic_reg() %>% 
  set_engine('glm') %>% 
    set_mode('classification')

logistic_fit = logistic_model %>% fit(purchased ~ total_visits + total_time, data = leads_training)
```

recipe() defines the preprocessing 
prep() calculates statistics from the training set 
bake() applies the preprocessing to data sets 

beans 
```{r}
library(beans)

bean_split = initial_split(beans, strata = class, propo = 3/4)
bean_train = training(bean_split)
bean_test = testing(bean_split)

bean_val = validation_split(bean_train, strata = class, prop = 4/5)
bean_val$splits[1]
```

basic recipe to preprocess the data prior to any dimensionality reduction steps: 
```{r}
library(bestNormalize)
bean_rec <-
  # Use the training data from the bean_val split object
  recipe(class ~ ., data = analysis(bean_val$splits[[1]])) %>%
  step_zv(all_numeric_predictors()) %>%
  step_orderNorm(all_numeric_predictors()) %>% 
  step_normalize(all_numeric_predictors())
```
prep for a recipe is like fit() for a model
```{r}
bean_rec_trained = prep(bean_rec)
bean_rec_trained

bean_validation <- bean_val$splits %>% pluck(1) %>% assessment()
bean_val_processed = bake(bean_rec_trained, new_data = bean_validation)
```


PCA in tidymodels 
```{r}
library(ggforce)

plot_validation_results <- function(recipe, dat = assessment(bean_val$splits[[1]])) {
  recipe %>%
    # Estimate any additional steps
    prep() %>%
    # Process the data (the validation set by default)
    bake(new_data = dat) %>%
    # Create the scatterplot matrix
    ggplot(aes(x = .panel_x, y = .panel_y, color = class, fill = class)) +
    geom_point(alpha = 0.4, size = 0.5) +
    geom_autodensity(alpha = .3) +
    facet_matrix(vars(-class), layer.diag = 2) + 
    scale_color_brewer(palette = "Dark2") + 
    scale_fill_brewer(palette = "Dark2")
}

bean_rec_trained %>%
  step_pca(all_numeric_predictors(), num_comp = 4) %>% 
  plot_validation_results() + 
  ggtitle("Principal Component Analysis")
```
source: <https://www.tmwr.org/dimensionality.html>










# USING TIDYMODELS on PINOT (our old friend)

```{r}
wine = wine %>% select(-cherry)

wine_words <- function(df, j = 1000, stem=F){ 
  library(tidytext)
  library(SnowballC)
  data(stop_words)

  words <- df %>%
    unnest_tokens(word, description) %>%
    anti_join(stop_words) %>% # get rid of stop words
    filter(!(word %in% c("wine","pinot","vineyard")))
  
  if(stem){
    words <- words %>% 
      mutate(word = wordStem(word))
  }
  
  words <- words %>% 
    count(id, word) %>% 
    group_by(id) %>% 
    mutate(exists = (n>0)) %>% 
    ungroup %>% 
    group_by(word) %>% 
    mutate(total = sum(n)) %>% 
    filter(total > j) %>% 
    pivot_wider(id_cols = id, names_from = word, values_from = exists, values_fill = list(exists=0)) %>% 
    right_join(select(df,id,province)) %>% 
    #drop_na() %>% 
    select(-id)
  
  
  words <- replace(words, is.na(words), FALSE)
}


winx = wine_words(wine, j = 1000)

bind_cols(wine %>% select(-description), winx %>% select(-province))

word_dummies = as.data.frame(lapply(winx %>% select(-province), as.numeric))

vin = bind_cols(wine %>% select(-id, -description, -province), word_dummies)

```


NOW TO DO SOME UNSUPERVISED LEARNING ON VIN!

## pca

```{r}
pca_rec <- recipe(~ ., data = vin) %>% 
  step_normalize(all_predictors()) %>% 
  step_pca(all_predictors(), num_comp = 6, id = "pca")

pca_estimates = prep(pca_rec)

pca_features = pca_estimates %>% bake(new_data = NULL)

pca_estimates %>% tidy(id = "pca", type = "variance") %>% filter(str_detect(terms, "percent"))
```
scale and center our features: 
```{r}
vin_features = recipe(~., data = vin) %>% 
  step_normalize(all_predictors()) %>% 
  prep() %>% 
  bake(new_data = NULL)
```

making tubby clusters: 
```{r}
kclusts = tibble(k = 1:10) %>% 
  mutate(
    model = map(k, ~ kmeans(x = vin_features, centers = .x, nstart = 20)),
    glanced = map(model, glance)) %>% 
  unnest(cols = c(glanced))
```


```{r}
kclusts %>% 
  ggplot(mapping = aes(x = k, y = tot.withinss)) +
  geom_line(size = 1.2, alpha = 0.5, color = "dodgerblue3") +
  geom_point(size = 2, color = "dodgerblue3")

# a very slight elbow around k = 5
```


```{r}
final_kmeans = kmeans(vin_features, centers = 5, nstart = 100, iter.max = 1000)

results = augment(final_kmeans, vin_features)
```



