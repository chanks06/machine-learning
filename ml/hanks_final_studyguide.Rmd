---
title: "hanks_final_prep"
output: pdf_document
date: "2023-04-13"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
options(scipen = 999)
```

# FINAL EXAM PREPARATION 

The purpose of this document is to consolidate all pertinent theory, code, and examples in one place to have on hand for final exam. The final exam is on Tuesday, April 18. 

wine_words function for text mining description: 
```{r}
wine_words <- function(df, j = 1000, stem=F){ 
  library(tidytext)
  library(SnowballC)
  data(stop_words)

  words <- df %>%
    unnest_tokens(word, description) %>%
    anti_join(stop_words) %>% # get rid of stop words
    filter(!(word %in% c("wine","pinot","vineyard")))
  
  if(stem){
    words <- words %>% 
      mutate(word = wordStem(word))
  }
  
  words <- words %>% 
    count(id, word) %>% 
    group_by(id) %>% 
    mutate(exists = (n>0)) %>% 
    ungroup %>% 
    group_by(word) %>% 
    mutate(total = sum(n)) %>% 
    filter(total > j) %>% 
    pivot_wider(id_cols = id, names_from = word, values_from = exists, values_fill = list(exists=0)) %>% 
    right_join(select(df,id,province)) %>% 
    #drop_na() %>% 
    select(-id)
  
  
  words <- replace(words, is.na(words), FALSE)
}
```

## 1. Feature Engineering
• Encoding categorical variables
• Interactions
• Transformations
• Standardizing

*Centering* is basically a technique where mean of independent variables is subtracted from all the values. It means all independent variables have zero mean. 

*Scaling* is similar to centering. Predictor variables are divided by their standard deviation. This way data will have a standard deviation of one.

Demonstrating what scale() does manually 
```{r}
#creating a data frame of random numbers 
df = data.frame(x = sample(1:1000,100), y = sample(1:1000,100))

#saving the average of x to a variable m:
m = mean(df$x)

#creating a column of centered x, where we subtract each value by the average of the col 
df = df %>% mutate(x.cent = x-m)

#dividing that column by the standard deviation 
df = df %>% mutate(x.cent.scaled = x.cent/sd(x.cent))

#comparing our manual scale and center x with the function scale() 
df= df %>% mutate(xsc = scale(x,center = TRUE, scale = TRUE), 
                  ysc = scale(y, center = TRUE, scale = TRUE)) 

#they are the same. 

#let's visualize this 
ggplot(df, aes(x = x, y = y)) + geom_point(color = "red") #very random!
ggplot(df, aes(x = xsc, y = ysc)) + geom_point(color = "blue")

```



## 2. KNN
• How does it work?
• Accuracy vs. Kappa
• Leakage
• Tuning
• Classification vs. Regression
• Confusion Matrix


## 3. Naive Bayes
• How does it work?
• What is a conditional probability?


## 4. Tidytext
• Document-term matrix
• tokens, stems


## 5. Logistic Regression
• How does it work?
• Why is it called regression and not classification?
• ROC curves
• Regularization


## 6. Decision Trees
• How do they work?
• What is information gain?
• What is the complexity parameter?
• What does weighting the classes accomplish?
• How do random forests work?
• What are the strengths / weaknesses?

Random forests models are solid for both regression and classification problems. They handle outliers well (as long as the training process includes cross validation). They are good for gather information on feature importance. There are multiple hyperparameters to tune get great performance. 

They can be computationally expensive, especially with a high ntree. 

• Random forest vs. bagged trees


## 7. Dimensionality Reduction
• What is curse of dimensionality?
• K-means
• Analysis of cluster centers, density plots
• Hierarchical
• Principle Component Analysis
• Factor loadings, interpretation, density plots
• Choosing number of clusters, choosing number of PCs to use


## 8. Clustering
• Cluster centers, interpretation
• Algorithm
• LDA: when and why?


## 9. Deep Learning
• Nodes
• Weights
• Activation Function

## 10. Meta-techniques
• Bagging
Bagging = "boostrap aggregating" 

Bagging combines and averages multiple models. Averaging across multiple trees reduces the variability of any one tree and reduces overfitting, which improves predictive performance. Bagging follows three simple steps:

Create m bootstrap samples from the training data. Bootstrapped samples allow us to create many slightly different data sets but with the same distribution as the overall training set.
For each bootstrap sample train a single, unpruned regression tree.
Average individual predictions from each tree to create an overall average predicted value.

Bagging provides the greatest improvement for models that have high variance. 

how to bag in caret: 
```{r}
ctrl <- trainControl(method = "cv",  number = 10) 

# CV bagged model
bagged_cv <- train(
  Sale_Price ~ .,
  data = ames_train,
  method = "treebag", #treebag 
  trControl = ctrl,
  importance = TRUE
  )
```



• Boosting: GBM, what is it and how does it differ from random forest / treebag?

## GBM 

A boosted gradient model is a type of machine learning model that uses an ensemble of weak prediction models, such as decision trees, to create a stronger, more accurate prediction model. The term "boosted" refers to the fact that each subsequent model in the ensemble is built on the errors of the previous models. 

you can get a feature importance plot from gbm using `summary(fit$finalModel, cbars = 10, method = relative.influence, las = 2)`

*XGBoost* is an optimization of standard gradient boosting. It has fancy techniques for
• approximating optimal splits of continuous features;
• parallel processing of model training;
• handling sparse matrices and smart use of memory and cores;
• sophisticated built-in regularization.

XGBoost only works with matrices that contain all numeric variables.

Whereas random forests build an ensemble of deep independent trees, GBMs build an ensemble of shallow and weak successive trees with each tree learning and improving on the previous.

Advantages and Disadvantages of Gradient Boosting Models:  

Advantages:

Often provides predictive accuracy that cannot be beat.
Lots of flexibility - can optimize on different loss functions and provides several hyperparameter tuning options that make the function fit very flexible.
No data pre-processing required - often works great with categorical and numerical values as is.
Handles missing data - imputation not required.

Disadvantages: 

GBMs will continue improving to minimize all errors. This can overemphasize outliers and cause overfitting. Must use cross-validation to neutralize.
Computationally expensive - GBMs often require many trees (>1000) which can be time and memory exhaustive.
The high flexibility results in many parameters that interact and influence heavily the behavior of the approach (number of iterations, tree depth, regularization parameters, etc.). This requires a large grid search during tuning.
Less interpretable although this is easily addressed with various tools (variable importance, partial dependence plots, LIME, etc.).

The main idea is to add new models to the ensemble sequentially. We use 'weak learners- that is, decision trees that are shallow (few splits). The benefits are: 

Speed: Constructing weak models is computationally cheap.
Accuracy improvement: Weak models allow the algorithm to learn slowly; making minor adjustments in new areas where it does not perform well. In general, statistical approaches that learn slowly tend to perform well.
Avoids overfitting: Due to making only small incremental improvements with each model in the ensemble, this allows us to stop the learning process as soon as overfitting has been detected (typically by using cross-validation).


• Ensembles: how and why?

many models where all the outcomes are averaged (regression) or plurality voted (classification)
## 11. Tidy Models
• Basic usage

```{r}
library(tidymodels)
#procedures for splitting train/test data 
mpg_split = initial_split(mpg, prop = 3/4, strata = hwy)

mpg_training = mpg_split %>% training()
mpg_test = mpg_split %>% testing()

```

- On the split = mapping: 
Since the original data is split at random, stratification avoids placing all the expensive homes in home_sales into the test dataset, for example. In this case, your model would most likely perform poorly because it was trained on less expensive homes.

linear regression model: 

parsnip: 
1. model type (linear regression, etc. )
2. set engine type 
3. specify the mode(either regression or classifcation)

```{r}
lm_model = linear_reg() %>% set_engine('lm') %>% set_mode('regression')
```

pass lm_model to the fit() function: 
```{r}
lm_fit = lm_model %>% fit(hwy ~ cty, data = mpg_training)
```

pass into tidy() to create summary table of coefficients 
```{r}
tidy(lm_fit)
```

make predictions using model: 
```{r}
hwy_predictions = lm_fit %>% predict(new_data = mpg_test)
```

add predictions to the test data 
```{r}
mpg_test_results = mpg_test %>% select(hwy, cty) %>% bind_cols(hwy_predictions)
```

Evaluating model performance:  
ALl yardstick functions require a tibble 
like this: 
```{r}
mpg_test_results
```
rmse and R^2
```{r}
mpg_test_results %>% rmse(truth = hwy, estimate = .pred)

mpg_test_results %>% rsq(truth = hwy, estimate = .pred)
```
last_fit()
```{r}
lm_last_fit = lm_model %>% last_fit(hwy ~ cty, split = mpg_split)

lm_last_fit %>% collect_metrics()

lm_last_fit %>% collect_predictions()
```

```{r}
# Create an R squared plot of model performance
ggplot(home_test_results, aes(x = selling_price, y = .pred)) +
  geom_point(alpha = 0.5) + 
  geom_abline(color = 'blue', linetype = 2) +
  coord_obs_pred() +
  labs(x = 'Actual Home Selling Price', y = 'Predicted Selling Price')
```

```{r}
# Define a linear regression model
linear_model <- linear_reg() %>% 
  set_engine('lm') %>% 
  set_mode('regression')

# Train linear_model with last_fit()
linear_fit <- linear_model %>% 
  last_fit(selling_price ~ ., split = home_split)

# Collect predictions and view results
predictions_df <- linear_fit %>% collect_predictions()
predictions_df
                                        
# Make an R squared plot using predictions_df
ggplot(predictions_df, aes(x = selling_price, y = .pred)) + 
  geom_point(alpha = 0.5) + 
  geom_abline(color = 'blue', linetype = 2) +
  coord_obs_pred() +
  labs(x = 'Actual Home Selling Price', y = 'Predicted Selling Price')
```

